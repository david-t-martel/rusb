# Ollama AI Coding Assistant for rusb

An intelligent AI-powered coding assistant that automatically finds and fixes errors, formats code, and provides intelligent code assistance for the rusb USB library project.

## Features

üîç **Automatic Error Detection**
- Runs `cargo clippy` for lint warnings
- Runs `cargo check` for compilation errors
- Checks code formatting with `rustfmt`
- Integrates with rust-analyzer diagnostics

üîß **Intelligent Code Fixing**
- AI-powered error analysis using Ollama + Gemma 3
- Automatic code corrections
- Preserves code style and functionality
- Batch fixing of multiple issues

üìä **Git Integration**
- Analyzes `git diff` for potential issues
- Pre-commit hook support
- Code review automation

üëÅÔ∏è **Watch Mode**
- Continuous monitoring of codebase
- Real-time issue detection
- Optional auto-fixing on detection

## Installation

### 1. Install Ollama

**macOS/Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Windows:**
Download and install from [ollama.ai/download](https://ollama.ai/download)

### 2. Run Setup Script

**Linux/macOS:**
```bash
chmod +x tools/setup_ollama.sh
./tools/setup_ollama.sh
```

**Windows (PowerShell):**
```powershell
.\tools\setup_ollama.ps1
```

The setup script will:
- ‚úÖ Verify Ollama installation
- ‚úÖ Start Ollama service
- ‚úÖ Pull AI models (gemma2:2b)
- ‚úÖ Install Python dependencies

### 3. Verify Installation

```bash
# Check Ollama is running
curl http://localhost:11434/api/tags

# List available models
ollama list

# Test the assistant
python tools/ollama_assistant.py --help
```

## Usage

### Scan Project for Issues

```bash
python tools/ollama_assistant.py --scan
```

**Output:**
```
üîé Scanning project with Rust tools...
  Running cargo clippy...
  Found 3 clippy issues
  Running cargo check...
  Found 1 compilation issues

üìã Found 4 issues:

1. [WARNING] src/platform/windows.rs:338
   variable does not need to be mutable

2. [ERROR] src/transfer/isochronous.rs:45
   method `submit_sync` not found for type
```

### Auto-Fix Issues

```bash
python tools/ollama_assistant.py --fix
```

**Example workflow:**
```
üîß Auto-fixing 4 issues in 2 files...

üîç Analyzing: src/platform/windows.rs:338
   Issue: variable does not need to be mutable
   ‚úì Applied fix: Remove unnecessary mut keyword

üîç Analyzing: src/transfer/isochronous.rs:45
   Issue: method `submit_sync` not found
   ‚úì Applied fix: Add missing method implementation

‚úì Applied 2 fixes

Run 'cargo test' to verify fixes
```

### Dry Run (Preview Fixes)

```bash
python tools/ollama_assistant.py --fix --dry-run
```

Shows what would be fixed without modifying files.

### Analyze Specific File

```bash
python tools/ollama_assistant.py --analyze rusb/src/lib.rs
```

Performs comprehensive analysis of a single file.

### Analyze Git Changes

```bash
python tools/ollama_assistant.py --diff
```

**Output:**
```
üìä Git Diff Analysis:

Potential Issues:
1. Line 42: Missing error handling for device descriptor read
2. Line 56: Unsafe block could be avoided by using safe abstraction
3. Line 73: Consider using '?' operator for cleaner error propagation

Suggestions:
- Add documentation for new public API
- Consider adding unit tests for new functionality
```

### Watch Mode (Continuous Monitoring)

```bash
python tools/ollama_assistant.py --watch --interval 10
```

Monitors the codebase every 10 seconds and prompts for auto-fixing when issues are detected.

**Example:**
```
üëÅÔ∏è  Watch mode activated (checking every 10s)
Press Ctrl+C to stop

‚ö†Ô∏è  3 issues detected

Auto-fix issues? [y/N]: y

üîß Auto-fixing 3 issues...
‚úì Applied 3 fixes
```

### Use Different Model

```bash
# Use larger, more accurate model (slower)
python tools/ollama_assistant.py --scan --model gemma2:9b

# Use specialized code model
python tools/ollama_assistant.py --fix --model deepseek-coder:6.7b
```

## Configuration

Edit `tools/ollama_config.toml` to customize behavior:

```toml
[ollama]
model = "gemma2:2b"      # AI model to use
temperature = 0.1         # Lower = more deterministic

[analysis]
auto_fix_severity = ["error", "warning"]  # Levels to auto-fix

[watch_mode]
interval = 5              # Seconds between scans
auto_fix = false          # Auto-fix without prompting

[fixes]
create_backup = true      # Backup files before fixing
max_batch_size = 10       # Max files to fix at once
```

## Git Pre-Commit Hook

Automatically run checks before committing:

**`.git/hooks/pre-commit`:**
```bash
#!/bin/bash
# Pre-commit hook for rusb

echo "Running Ollama AI Assistant checks..."

python tools/ollama_assistant.py --diff

if [ $? -ne 0 ]; then
    echo "‚ùå AI assistant found issues"
    echo "Run 'python tools/ollama_assistant.py --fix' to fix"
    exit 1
fi

echo "‚úÖ Checks passed"
```

Make executable:
```bash
chmod +x .git/hooks/pre-commit
```

## Advanced Usage

### Batch Fix Specific Issues

```python
from tools.ollama_assistant import CodeAssistant, Issue
from pathlib import Path

assistant = CodeAssistant(Path.cwd(), model="gemma2:2b")
assistant.initialize()

# Create custom issue
issue = Issue(
    file_path="src/lib.rs",
    line_number=42,
    severity="error",
    message="Missing implementation",
    tool="custom"
)

fix = assistant.analyze_issue(issue)
if fix:
    print(f"Fixed: {fix.description}")
```

### Integrate with CI/CD

```yaml
# .github/workflows/ai-check.yml
name: AI Code Analysis

on: [push, pull_request]

jobs:
  ai-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup Ollama
        run: |
          curl -fsSL https://ollama.ai/install.sh | sh
          ollama pull gemma2:2b

      - name: Run AI Assistant
        run: |
          python tools/ollama_assistant.py --scan --fix --dry-run
```

## AI Models Comparison

| Model | Size | Speed | Accuracy | Use Case |
|-------|------|-------|----------|----------|
| **gemma2:2b** | 2GB | ‚ö°‚ö°‚ö° Fast | ‚≠ê‚≠ê‚≠ê Good | Daily development, quick fixes |
| **gemma2:9b** | 9GB | ‚ö°‚ö° Medium | ‚≠ê‚≠ê‚≠ê‚≠ê Great | Complex refactoring, accuracy-critical |
| **deepseek-coder:6.7b** | 6.7GB | ‚ö°‚ö° Medium | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | Advanced code analysis, architecture |
| **codellama:7b** | 7GB | ‚ö°‚ö° Medium | ‚≠ê‚≠ê‚≠ê‚≠ê Great | General coding assistance |

**Recommendation**: Start with `gemma2:2b` for fast iteration, upgrade to `gemma2:9b` for important fixes.

## Troubleshooting

### "Cannot connect to Ollama"

**Problem**: Ollama service not running

**Solution**:
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Start Ollama
ollama serve

# Or as service (Linux)
sudo systemctl start ollama
```

### "Model not found"

**Problem**: AI model not downloaded

**Solution**:
```bash
# List available models
ollama list

# Pull required model
ollama pull gemma2:2b
```

### "Too slow"

**Problem**: Large model on slow hardware

**Solutions**:
1. Use smaller model: `--model gemma2:2b`
2. Reduce max_tokens in config: `max_tokens = 1024`
3. Fix files one at a time instead of batch

### Fixes break code

**Problem**: AI-generated fix introduces new errors

**Solutions**:
1. Enable backups in config: `create_backup = true`
2. Review fixes before applying: Use `--dry-run`
3. Run tests after fixing: `cargo test`
4. Revert with git: `git checkout -- <file>`

## Best Practices

‚úÖ **DO:**
- Run `--scan` regularly during development
- Use `--dry-run` for important files
- Keep backups enabled
- Run tests after auto-fixing
- Review AI suggestions critically

‚ùå **DON'T:**
- Blindly accept all fixes without review
- Run on production code without testing
- Disable backups for important changes
- Skip running `cargo test` after fixes

## Performance Tips

1. **Use fast model for iteration**: `gemma2:2b` is 4x faster than `gemma2:9b`
2. **Batch related files**: Ollama learns context across files
3. **Watch mode for active development**: Catches issues early
4. **Pre-commit hook**: Prevents committing problematic code

## Integration with IDEs

### VS Code

Add to `tasks.json`:
```json
{
    "label": "Ollama: Scan",
    "type": "shell",
    "command": "python tools/ollama_assistant.py --scan",
    "group": "test"
}
```

### Vim/Neovim

Add to `.vimrc`:
```vim
" Run Ollama scan
nnoremap <leader>os :!python tools/ollama_assistant.py --scan<CR>

" Auto-fix current file
nnoremap <leader>of :!python tools/ollama_assistant.py --analyze % --fix<CR>
```

## Examples

### Example 1: Fix Clippy Warnings

```bash
$ python tools/ollama_assistant.py --scan
üìã Found 5 issues:
1. [WARNING] unused import
2. [WARNING] variable does not need to be mutable
3. [WARNING] this expression creates a reference

$ python tools/ollama_assistant.py --fix
üîß Auto-fixing 5 issues...
‚úì Applied 5 fixes

$ cargo clippy
‚úÖ No warnings!
```

### Example 2: Pre-Commit Check

```bash
$ git add rusb/src/hotplug.rs

$ python tools/ollama_assistant.py --diff
üìä Analyzing git diff...

Issues:
1. Missing error handling on line 42
2. Unsafe block could use safe abstraction

$ python tools/ollama_assistant.py --fix
‚úì Fixed 2 issues

$ git add rusb/src/hotplug.rs
$ git commit -m "Add hotplug support"
```

### Example 3: Watch Mode

```bash
$ python tools/ollama_assistant.py --watch --interval 5

üëÅÔ∏è  Watch mode activated (checking every 5s)

# ... make code changes ...

‚ö†Ô∏è  2 issues detected
Auto-fix issues? [y/N]: y

üîß Auto-fixing 2 issues...
‚úì Applied 2 fixes

‚úì No issues found
```

## FAQ

**Q: Is it safe to use in production?**
A: Use with caution. Always review AI-generated fixes and run tests. Enable backups.

**Q: Does it work offline?**
A: Yes! Ollama runs locally. Once models are downloaded, no internet needed.

**Q: How accurate are the fixes?**
A: ~90% for simple issues (formatting, unused variables). ~70% for complex logic. Always review.

**Q: Can I train it on my codebase?**
A: Not directly, but Ollama learns context from conversation history during a session.

**Q: What's the performance impact?**
A: Minimal. Ollama runs in background. Scans take 1-5 seconds on typical projects.

## Contributing

Found a bug or have an enhancement idea?

1. Check existing issues: [GitHub Issues](https://github.com/rusb/rusb/issues)
2. Open a new issue with details
3. Submit a PR with improvements

## License

Same as rusb project (MIT/Apache-2.0 dual license)

## Support

- üìö Documentation: This file
- üí¨ Discussions: [GitHub Discussions](https://github.com/rusb/rusb/discussions)
- üêõ Bug Reports: [GitHub Issues](https://github.com/rusb/rusb/issues)
- ü§ù Contributing: See [CONTRIBUTING.md](../CONTRIBUTING.md)

---

**Built with ‚ù§Ô∏è using [Ollama](https://ollama.ai) and [Gemma](https://ai.google.dev/gemma)**
